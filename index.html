<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body bgcolor="white">
 <title> Mei-Yuh Hwang, speech recognition, machine translation, language understanding, scene-text OCR, 
  image recognition, GenAI </title>

<h3> Mei-Yuh Hwang 黄美玉</h3> 
<img src="CMU-pannel-2017.jpg" height=150>  <br>
<font size="-1"> <a href="https://www.ece.uw.edu/people/mei-yuh-hwang/">Affiliate Professor at EE Department </a> 
     </font> <br>
<font size="-1"> University of Washington  (UW) </font> <br>
 <font size="-1"> IEEE Fellow </font> <br>
   <font size="-1"> Reach me at my-svi on live.com </font> <br>
 
<p> Mei-Yuh received her PhD in Computer Science from Carnegie Mellon University in 1993 and have worked in AI industry/academia
for three decades,
<!-- at Microsoft for 20 years in U.S. 
  and in China, University of Washington for 4 years, and Mobvoi startup for 4 years, and Meta for a year, --> 
 publishing numerous conference and 
 journal papers, and delivering industry products in speech recognition, machine translation,
 language understanding, and image representation & recognition.

 Mei-Yuh's focus has been always on turning state-of-the-art technologies into end-users' hands. 
 She is an IEEE fellow, who is passionate in bridging the gap between academia and industry.

 <h3> <a href="https://vinbrain.net/">Vinbrain</a>, 3/2022-11/2023 </h3>
 <ul>
 <li>Technical advisor, remote </li>
  <li> Attendee, <a href="https://www.rsna.org/annual-meeting">RSNA</a> 2023 </li>
 </ul>
 <h3> Meta, 5/2022--5/2023 </h3>
 <ul>
  <li> AI Research Scientist, Bellevue WA
   <li> Speech recognition patent: Device wake-up modeling using a word-piece model.
  <li> <a href="https://en.wikipedia.org/wiki/Scene_text">Scene-Text OCR</a> 
   <ul>
    <li> <a href="https://arxiv.org/abs/2308.13173">DISGO: Automatic End-to-End Evaluation for Scene Text OCR</a>: Evaluating machine translation from scene-text OCR output </li>
   </ul>
   <li> Automatic data filtering for training & evaluating text-to-image generation </li>
</ul>


 <h3> Microsoft, 4/2020-4/2022</h3>
 <ul>
 <li> Partner science manager, Microsoft Search for Office 365, Bellevue WA.
<li> NLP for Outlook and Teams search, based on fine-tuning on, and/or few-shot prompting to, various large pre-trained language models.
 <li> CTO winning project: Prompt engineering on GPT-3 copilot to manipulate Excel spreadsheets via human natural languages.
  <br>
  See related post in <a href="https://medium.com/data-science-at-microsoft/building-gpt-3-applications-beyond-the-prompt-504140835560">GPT-3 application</a>.
</ul>
 
<h3>Mobvoi AI Lab, 2016-2020</h3>
 <ul>
 <li> Director, Redmond WA.
<li>
<a href="http://www.mobvoi.com">Mobvoi</a> makes speech-enabled 
smart IoT
devices</a>, from hardware to software, all in-house.
 <ul>
 <li> We are one of the most successful companies in <a href="http://store.ticwear.com">smart watches</a> in China.
<li>  Mobvoi also provides the in-car voice assistant for VW automobiles in China.
 The technology includes both on-the-cloud and on-device voice navigation, enabling majority of services without internet connection.
<li> We are active in generative AI and have been very successful in personalized TTS. </li>
<li> Though a young company
that focuses on industry products, we are actively participating in
our speech research community with our limited resources. 

<!-- Our publications can be found in <a href="mobvoi/index.html#pub"> Mobvoi publications</a>. 
 <li> Mobvoi IPO news
  <ul>
     <li> <a href="https://news.bloomberglaw.com/mergers-and-acquisitions/chinese-chatgpt-rival-said-to-pick-banks-for-300-million-hk-ipo">Bloomberglaw.com</a> 
    <li> <a href="https://mp.weixin.qq.com/s/J0pAJDH44jGAMe_HOeN3iQ">IPO 早知道</a> 
  </ul>
 -->
</ul>

</ul>
 
<h3>Microsoft China,  2012-2015</h3>
 <ul>
 <li> Principal science manager, Spoken language understanding for Cortana, Beijing and Suzhou.
<li> Non-English Cortana
 <ul>
 <li> To deliver non-English Cortana without human annotated data,
Mei-Yuh designed
an adapted translation algorithm which
offered both paraphrasing and generalization capabilities
with required semantic slot tags. 
<li> The protoype model was further improved via
 iterative data augmentation using RNN and newly logged data.
 </ul>
 
 
<!-- her team delivered six languages of Cortana
language understanding models with above 90% F1 scores.
The impressive success of Chinese Cortana gained much attention within China, and sparked the development of 
personal assistants and AI across China. -->
 
  <li>Microsoft cognitive services on Azure cloud:
<ul>
<li> Cortana non-English language
understanding for  <a href="https://azure.microsoft.com/en-us/services/cognitive-services/language-understanding-intelligent-service/">LUIS</a>, 
<li> Language-model adaptation for
<a href="https://azure.microsoft.com/en-us/services/cognitive-services/custom-speech-service/">customized speech recognition</a>.
<li> Spontaneous 
speech recogition for
<a href="https://www.techlicious.com/blog/skype-translator-preview-app-launch/">Skype speech-to-speech translation</a>.
</ul>
</ul>


<h3>Microsoft and UW, 1994-2012</h3>
 <ul>
 <li> Machine translation, Microsoft, Redmond WA, 2008-2012.
  <ul>
<li>Co-built Bing Translator automated training infrastructure,
including the design and implementation of 
 map-reduce parallel processing, based on <a href="https://www.microsoft.com/en-us/research/project/dryadlinq/">DryadLink</a>.
<li>Designed and implemented
<a href="https://hub.microsofttranslator.com/">
 Bing Translation Hub</a> for customized vertical-domain translation.
  </ul>
  
 <li> Speech recognition at Univ. of Washington, Seattle WA, 2004-2008.
  <ul> <li>
 Led the DARPA EARS and GALE Mandarin speech recongition projects
at University of Washington.
   <li> Won the best Mandarin speech recognition in 2007.
  </ul>


<li> Speech recognition, Microsoft Research & Production, Redmond WA, 1994-2004.
 <ul> <li>
 Ported Sphinx-II speech recognition to Microsoft on Windows desktop, Office, and Microsoft Speech Server SDK,
for the recognition of multiple languages. 
</ul>
 </ul>
 
<h3>SPHINX-II speech recognition at CMU, PA, 1987-1993</h3>
 <ul>
 <li> First to propose
Markov state clustering, based on decision trees, for continuous speech recognition. 
  <li> The idea
of shared states (or
<a href="https://ieeexplore.ieee.org/document/225979/">senones</a> as Mei-Yuh named it in 1992) had
 been widely adopted for two decades since its inception, until recent years when end-to-end neural-transducer based 
  and transformer-based speech recognition took over a new era.
  <li> Participated in numerous DARPA speech recognition evaluation benchmarks (Resource Management, WSJ, ATIS) 
   and won the top position consistently.
 </ul>

<!--
<h3>Innovation</h3>
During all the years at industry, Mei-Yuh stays on top of research and works with researchers
side by side. She co-authored dozens of patents that she designed to solve practical
problems she encountered while delivering products. She stays active and contributive
to the research community.
Now she is combining all of her expertize
at <a href="http://www.mobvoi.com">Mobvoi</a> AI Lab 
-->

<h3>Awards</h3>
<ul>
 <li> 2023, <a href="outstanding2023.pdf">Outstanding Reviewer, ICASSP 2023 </a>
 </li>
 <li> 2021, <a href="https://www.aaia-ai.org/fellows?page=58">AAIA Fellow</a>
<li>2019, IEEE Fellow
<li> 2010, Microsoft Gold Star Award from Microsoft Research, Redmond, WA
<li> 1992, Allen Newell Research Excellence Medal, Pittsburgh, Carnegie Mellon University
<li> 1986, <a href="http://www.phitauphi.org.tw/">Phi Tao Phi Scholastic Honor Society</a>, recommended by National Taiwan University
</ul>

<h3>Professional Services</h3>
<ul>
<li> 2015-2018: IEEE ISCSLP steering committee
<li> 2013: IEEE associate editor for Transactions on Audio, Speech, and Language Processing (ASLP)
<li> 2011: Technical Chair of IWSLT
<li> 1998: Publicity Chair IEEE ICASSP
<li> Reviewers for IEEE Transactions on ASLP
<li> Technical committee for ICASSP, Interspeech, ISCSLP, ACL, NAACL</li>
</ul>

<h3> Invited Talks</h3>
<ul>
<li> 2020: WeCNLP Summit, Seattle online.
<li> 2019: Northwestern Polytechnical University, Xi-an, China
<li> 2018 April: Yuanchuan Telephony company, Taiwan
<li> 2017: <a href="https://www.youtube.com/watch?v=iVnBcGXBs3w">UWEE
Research Colloquium Talk</a>
<li> 2017: Panelist, <a href="https://cmu-summit.com/2017-sixth-summit/">
CMU Summit</a>
<li> 2017: Talk at Southwest Forestry Univerisity, Kunming, China
<li> 2017: Talk at Soochow University, Suzhou, China
<li> 2015: Talk at Soochow University, Suzhou, China
<li> 2014: Talk at University of Science and Technology of China, Suzhou campus
<li> 2014: Keynote speech at PhD Forum, Northwestern Polytechnical University, Xi-an, China
<li> 1994-2003: Lecturer at National Taiwan University, Academia Sinica, and ITRI, Taiwan
<li> 1993: Lecturer at IBM Gaithersburg
</ul>

<h3><a href="https://scholar.google.com/citations?hl=en&user=Z_yucaMAAAAJ">Google Scholar</a></h3>
 
<h3>Education</h3>
<ul>
<li>PhD, Computer Science, Carnegie Mellon University, December 1993.
<!--
<ul>
Thesis: <a href="pub/1993/thesis.pdf">Subphonetic modeling for speech recogniton --- Senones</a>
</ul> -->
<li>BA, Computer Science, National Taiwan University (台大), June 1986.
</ul>

 <img src="mei-2024.jpg" height=150>  <br>
 
<!--
<p>Mei-Yuh's <a href="index_files/children.htm">personal home page</a>
-->
</html>

